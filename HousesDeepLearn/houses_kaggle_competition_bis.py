# -*- coding: utf-8 -*-
"""houses_kaggle_competition_bis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14zUrTl_8lDgGMsOPVdhK7sr__XRfSrJp

# Houses Kaggle Competition

[<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/ML/kaggle-batch-challenge.png' width=600>](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)

⚙️ Let's re-use our previous **pipeline** built in the module **`05-07-Ensemble-Methods`** and try to improve our final predictions with a Neural Network!

## (0) Libraries and imports
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

# DATA MANIPULATION
import pandas as pd
pd.set_option('display.max_columns', None)
import numpy as np

# DATA VISUALISATION
import matplotlib.pyplot as plt
import seaborn as sns

# VIEWING OPTIONS IN THE NOTEBOOK
from sklearn import set_config; set_config(display='diagram')

"""## (1) 🚀 Getting Started

### (1.1) Load the datasets

💾 Let's load our **training dataset**
"""

data = pd.read_csv("https://wagon-public-datasets.s3.amazonaws.com/houses_train_raw.csv")
X = data.drop(columns='SalePrice')
y = data['SalePrice']

X.head()

X.shape, y.shape

"""💾 Let's also load the **test set**

❗️ Remember ❗️ You have access to `X_test` but only Kaggle has `y_test`
"""

X_test = pd.read_csv("https://wagon-public-datasets.s3.amazonaws.com/houses_test_raw.csv")

X_test.shape

"""### (1.2) Train/Val Split

❓ **Holdout** ❓

As you are not allowed to use the test set (and you don't have access to `y_test` anyway), split your dataset into a training set and a validation set.
"""

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3)
X_train.shape, X_val.shape, y_train.shape, y_val.shape

"""### (1.3) Import the preprocessor"""

!tree

"""🎁 You will find in `utils/preprocessor.py` the **`data-preprocessing pipeline`** that was built in our previous iteration.

❓ Run the cell below, and make sure you understand what the pipeline does. Look at the code in `preprocessor.py` ❓
"""

from preprocessor import create_preproc

preproc = create_preproc(X_train)
preproc

"""❓ **Scaling your numerical features and encoding the categorical features** ❓

Apply these transformations to _both_ your training set and your validation set.
"""

# Fit the preprocessor on the train set
preproc.fit(X_train, y_train)

# Create the preprocessed versions of X_train and X_val
X_train_preproc = preproc.transform(X_train)
X_val_preproc = preproc.transform(X_val)

# Let's also already create the preprocessed version of X_test for our future predictions
X_test_preproc = preproc.transform(X_test)

# Shapes before preprocessing
X_train.shape, X_val.shape, X_test.shape

# Shapes after preprocessing
X_train_preproc.shape, X_val_preproc.shape, X_test_preproc.shape

"""## (2) 🔮 Your predictions in Tensorflow/Keras

🚀 This is your first **regression** task with Keras!

💡 Here a few tips to get started:
- Kaggle's [rule](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) requires to minimize **`rmsle`** (Root Mean Square Log Error).
    - As you can see, we can specify `msle` directly as a loss-function with Tensorflow.Keras!
    - Just remember to take the square-root of your loss results to read your rmsle metric.
    
    
😃 The best boosted-tree ***rmsle*** score to beat is around ***0.13***

---

<img src="https://i.pinimg.com/564x/4c/fe/ef/4cfeef34af09973211f584e8307b433c.jpg" alt="`Impossible mission" style="height: 300px; width:500px;"/>

---


❓ **Your mission, should you choose to accept it:** ❓
- 💪 Beat the best boosted-tree 💪

    - Your responsibilities are:
        - to build the ***best neural network architecture*** possible,
        - and to control the number of epochs to ***avoid overfitting***.

### (2.1) Predicting the houses' prices using a Neural Network

❓ **Preliminary Question: Initializing a Neural Network** ❓

Create a function `initialize_model` which initializes a Dense Neural network:
- You are responsible for designing the architecture (number of layers, number of neurons)
- The function should also compile the model with the following parameters:
    - ***optimizer = "adam"***
    - ***loss = "msle"*** (_Optimizing directly for the Squared Log Error!_)
"""

from tensorflow.keras import Sequential, layers

def initialize_model(X):

    #################################
    #  1 - Model architecture       #
    #################################

    model = Sequential()

    # Input Layer
    model.add(layers.Dense(20, activation='relu', input_dim = X.shape[-1]))

    # Hidden Layers
    model.add(layers.Dense(15, activation='relu'))
    model.add(layers.Dense(15, activation='relu'))
    model.add(layers.Dense(20, activation='relu'))

    # Predictive Layer
    model.add(layers.Dense(1, activation='linear'))

    ##################################
    #  2 - Our recommended compiler  #
    ##################################

    model.compile(optimizer='adam',
                  loss='msle')      # directly optimize for the squared log error!

    return model

"""❓ **Questions/Guidance** ❓

1. Initialize a Neural Network
2. Train it
3. Evaluate its performance
4. Is the model overfitting the dataset?
"""

# 1. Initializing a NeuralNet with its architecture and its compilation method
model = initialize_model(X_train_preproc)
model.summary()


# 2. Training the model
epochs = 500
batch_size = 16

history = model.fit(X_train_preproc,
                    y_train,
                    validation_data = (X_val_preproc, y_val),
                    epochs = epochs,         # Play with this until your validation loss overfit
                    batch_size = batch_size, # Let's keep a small batch size for faster iterations
                    verbose = 0)

# 3. Evaluating the model
res = model.evaluate(X_val_preproc, y_val, verbose = 0)
print(f"RMLSE achieved after {epochs} epochs = {round(res**0.5,3)}")

# 4. Looking at the lowest loss
minimium_rmlse_val = min(history.history['val_loss'])**0.5
optimal_momentum = np.argmin(history.history['val_loss'])

print(f"Lowest RMLSE achieved = {round(minimium_rmlse_val,3)}")
print(f"This was achieved at the epoch number {optimal_momentum}")

"""🎁 We coded a `plot_history` function that you can use to detect overfitting"""

def plot_history(history):
    plt.plot(np.sqrt(history.history['loss']))
    plt.plot(np.sqrt(history.history['val_loss']))
    plt.title('Model Loss')
    plt.ylabel('RMSLE')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'], loc='best')
    plt.show()

plot_history(history)

"""💪 No overfitting in this model...

🤔 However, was it worth running the model for 500 epochs when it had already converged after 100 epochs?

📆 More on that during ***Deep Learning > 02. Optimizers, Loss and Fitting***, stay tuned!

### (2.2) Challenging yourself

🤔 **Questions to challenge yourself:**
- Are you satisfied with your score?
- Before publishing it, ask yourself whether you could really trust it or not?
- Have you cross-validated your neural network?
    - Feel free to cross-validate it manually with a *for loop* in Python to make sure that your results are robust against the randomness of a _train-val split_ before before submitting to Kaggle

❓ Create a function `evaluate_model` following the framework below 👇 then use a for loop with `KFold` to manually cross validate your model!
"""

def evaluate_model(X, y, train_index, val_index):

    # Slicing the training set and the validation set
    X_train, X_val = X.iloc[train_index], X.iloc[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # Preprocessing
    preproc = create_preproc(X_train)
    preproc.fit(X_train, y_train)
    X_train_preproc = preproc.transform(X_train)
    X_val_preproc = preproc.transform(X_val)

    # Training the model on the preprocessed training dataset
    model = initialize_model(X_train_preproc)

    history = model.fit(X_train_preproc,
                        y_train,
                        validation_data = (X_val_preproc, y_val),
                        epochs = 500,
                        batch_size = 16,
                        verbose=0)

    # Evaluating the model on the preprocessed validation dataset

    return pd.DataFrame({
                'rmsle_final_epoch': [model.evaluate(X_val_preproc, y_val)**0.5],
                'rmsle_min': [min(history.history['val_loss'])**0.5]
                        })

from sklearn.model_selection import KFold

cv = 5
kf = KFold(n_splits = cv, shuffle = True)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# results = []
# 
# fold_number = 1
# 
# for train_index, val_index in kf.split(X):
#     print("-"*100)
#     print(f"FOLD NUMBER {fold_number}")
#     results.append(evaluate_model(X, y, train_index, val_index))
#     fold_number +=1
# 
# final_results = pd.concat(results, axis = 0)
# final_results.index = np.arange(0, len(results))

"""### (2.3) (Bonus) Using all your CPU cores to run Neural Networks

🔥 **BONUS** 🔥 **Multiprocessing computing using [dask](https://docs.dask.org/en/latest/delayed.html)** and **all your CPU cores**:

_(to mimic SkLearn's `n_jobs=-1`)_
"""

!pip install --quiet dask

from sklearn.model_selection import KFold
from dask import delayed

cv = 5
kf = KFold(n_splits = cv, shuffle = True)
f = delayed(evaluate_model)

results = delayed([f(X, y, train_index, val_index) for (train_index, val_index) in kf.split(X)
                  ]).compute(
                      scheduler='processes', num_workers=8)

pd.concat(results, axis=0).reset_index(drop=True)

"""### (2.4) (Bonus) Multiprocessing with Python

**multiprocessing with default Python library**

References :
* [Yitong Ren - Speeding Up and Perfecting Your Work Using Parallel Computing](https://towardsdatascience.com/speeding-up-and-perfecting-your-work-using-parallel-computing-8bc2f0c073f8)
* [Johaupt Github - Parallel Processing for Cross Validation - BROKEN LINK](https://johaupt.github.io/python/parallel%20processing/cross-validation/multiprocessing_cross_validation.html)
"""

# # This code will fail try to debug it yourself if you cannot checkout the hints below
# import multiprocessing as mp
# pool = mp.Pool(processes=2)

# results = []
# def log_result(x):
#     results.append(x)

# for train_index, val_index in kf.split(X):
#     pool.apply_async(
#         evaluate_model,
#         args=(X, y, train_index, val_index),
#         callback = log_result)

# # Close the pool for new tasks
# pool.close()

# # Wait for all tasks to complete at this point
# pool.join()

# result = pd.concat(results, axis=0)

# import multiprocessing
# import pandas as pd

# results = []
# def log_result(x):
#     results.append(x)

# num_processes = multiprocessing.cpu_count()
# pool = multiprocessing.Pool(processes=num_processes)

# for train_index, val_index in kf.split(X):
#     pool.apply_async(
#         evaluate_model,
#         args=(X, y, train_index, val_index),
#         callback=log_result)

# pool.close()

# pool.join()

# result = pd.concat(results, axis=0)

"""## (3) 🏅FINAL SUBMISSION

🦄 Predict the ***prices of the houses in your test set*** and submit your results to Kaggle!
"""

predictions = model.predict(X_test_preproc)
predictions

"""💾 Save your predictions in a Dataframe called `results` with the format required by Kaggle so that when you export it to a `.csv`, Kaggle can read it."""

results = pd.concat([
                        X_test["Id"],
                        pd.Series(predictions[:,0], name="SalePrice")
                    ],
                    axis=1)
results

"""📤  Export your results using Kaggle's submission format and submit it online!

_(Uncomment the last cell of this notebook)_
"""

results.to_csv("submission_final.csv", header = True, index = False)

"""---

🏁 Congratulations!

💾 Don't forget to `git add/commit/push` your notebook...

🚀 ... it's time for the Recap!
"""